{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjtoxcKd5kgjauoiu8cr84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidofitaly/notes_02_50_key_stats_ds/blob/main/06_chapter/01_statistical_machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table of contents:\n",
        "1. [k-Nearest Neighbors (k-NN)](#0)\n",
        "2. [Decision Trees  ](#1)\n",
        "3. [Bagging and Random Forests](#2)\n"
      ],
      "metadata": {
        "id": "PFKXwiQT647w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<a name='0'></a> k-Nearest Neighbors (k-NN)"
      ],
      "metadata": {
        "id": "NFrScaEnCNdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**k-Nearest Neighbors (k-NN)** is a simple and effective classification and regression algorithm based on the idea that similar data points tend to be close to each other.\n",
        "\n",
        "##### 1. Algorithm  \n",
        "For a given data point:\n",
        "1. Calculate the distance to all other points (e.g., **Euclidean distance**):\n",
        "   $$\n",
        "   d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
        "   $$\n",
        "2. Select the **k** nearest neighbors.\n",
        "3. Assign the class label based on majority voting (**classification**) or compute the average value (**regression**).\n",
        "\n",
        "##### 2. Choosing \\( k \\)  \n",
        "- Small \\( k \\) → **More sensitive** to noise, low bias, high variance.  \n",
        "- Large \\( k \\) → **Smoother** decision boundary, high bias, low variance.\n",
        "\n",
        "##### 3. Distance Metrics  \n",
        "Common distance metrics:\n",
        "- **Euclidean Distance**: Default metric for continuous data.\n",
        "- **Manhattan Distance**: Sum of absolute differences.\n",
        "- **Minkowski Distance**: Generalized version of Euclidean and Manhattan.\n",
        "\n"
      ],
      "metadata": {
        "id": "mjbNaMPQCIOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name='1'></a>Decision Trees  "
      ],
      "metadata": {
        "id": "UliIegufzUu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Decision trees** are a popular classification and regression technique that models decisions based on a tree-like structure. They are easy to interpret and can handle both numerical and categorical data.  \n",
        "\n",
        "##### Key Concepts  \n",
        "\n",
        "1. **Entropy and Information Gain**  \n",
        "   Entropy measures the impurity of a dataset. Lower entropy means purer data. Information gain measures the reduction in entropy after splitting a node.  \n",
        "   $$ H(S) = - \\sum p_i \\log_2 p_i $$  \n",
        "   where $p_i$ is the probability of each class in set $S$.  \n",
        "\n",
        "2. **Gini Index**  \n",
        "   An alternative to entropy, the Gini index measures the probability of incorrect classification.  \n",
        "   $$ G(S) = 1 - \\sum p_i^2 $$  \n",
        "\n",
        "3. **Decision Rule (Splitting Criteria)**  \n",
        "   The decision tree selects the best attribute to split on by maximizing information gain or minimizing the Gini index.  \n",
        "\n",
        "4. **Tree Pruning**  \n",
        "   Pruning reduces overfitting by removing unnecessary branches. It can be **pre-pruning** (stopping early) or **post-pruning** (trimming after training).  \n",
        "\n",
        "5. **Feature Importance**  \n",
        "   Features contributing most to classification are ranked by their role in reducing impurity.  \n",
        "\n"
      ],
      "metadata": {
        "id": "H2zacT8azRCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<a name='2'></a> Bagging and Random Forests  "
      ],
      "metadata": {
        "id": "8WkNBMZT3gT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Bagging (Bootstrap Aggregating)** is an ensemble learning method that improves model stability and accuracy by training multiple models on different bootstrap samples of the data and averaging their predictions.  \n",
        "\n",
        "##### 1.Bagging Process  \n",
        "1. Draw multiple bootstrap samples (random samples with replacement) from the dataset.  \n",
        "2. Train a separate model on each sample.  \n",
        "3. Aggregate predictions (average for regression, majority vote for classification).  \n",
        "\n",
        "Bagging reduces variance and prevents overfitting, making it effective for high-variance models like decision trees.  \n",
        "\n",
        "##### 2.Random Forests  \n",
        "A **random forest** is an extension of bagging where multiple decision trees are trained, but with an additional step:  \n",
        "- At each split, only a random subset of features is considered, reducing correlation between trees.  \n",
        "\n"
      ],
      "metadata": {
        "id": "R9h1xZeq3d5J"
      }
    }
  ]
}