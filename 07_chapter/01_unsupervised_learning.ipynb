{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORc4MTq/wd22mqSnucuGg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidofitaly/notes_02_50_key_stats_ds/blob/main/07_chapter/01_unsupervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table of contents:\n",
        "1. [Principal Component Analysis (PCA) ](#0)\n",
        "2. [K-Means Clustering  ](#1)\n",
        "3. [Hierarchical Clustering](#2)"
      ],
      "metadata": {
        "id": "JwgY8V7a9oj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name='0'></a>Principal Component Analysis (PCA)  "
      ],
      "metadata": {
        "id": "wAUCtCJu05jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**PCA (Principal Component Analysis)** is a dimensionality reduction technique that transforms data into a new space while preserving as much information as possible. It simplifies data, removes redundancy, and improves model efficiency.  \n",
        "\n",
        "##### Key Steps of PCA:  \n",
        "1. **Standardizing the Data** – ensuring all features have the same scale.  \n",
        "2. **Computing the Covariance Matrix** – analyzing feature co-variability.  \n",
        "3. **Calculating Eigenvalues and Eigenvectors** – identifying the main directions of variability.  \n",
        "4. **Selecting Principal Components** – choosing the most important components based on variance.  \n",
        "5. **Projecting Data** – transforming data into a lower-dimensional space.  \n",
        "\n",
        "##### Choosing the Number of Components  \n",
        "- **Explained Variance:** higher cumulative variance means more information retained.  \n",
        "- **Kaiser’s Rule:** keep components with eigenvalues > 1.  \n",
        "- **Scree Plot:** identifies the \"elbow point\" in the variance curve.  \n"
      ],
      "metadata": {
        "id": "TL1UVx86029d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name='1'></a> K-Means Clustering  "
      ],
      "metadata": {
        "id": "TOPWlKYO5T-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**K-Means** is an unsupervised clustering algorithm that partitions data into **K clusters** based on similarity. It minimizes the variance within clusters while maximizing the difference between them.  \n",
        "\n",
        "##### Algorithm Steps:  \n",
        "1. **Choose K** – the number of clusters.  \n",
        "2. **Initialize Centroids** – randomly select K points as initial cluster centers.  \n",
        "3. **Assign Points to Clusters** – each point is assigned to the nearest centroid.  \n",
        "4. **Update Centroids** – compute the mean of each cluster and update centroids.  \n",
        "5. **Repeat Steps 3-4** until centroids no longer change.  \n",
        "\n",
        "##### Choosing the Optimal K:  \n",
        "- **Elbow Method:** Plot the within-cluster sum of squares (WCSS) and look for the \"elbow point.\"  \n",
        "- **Silhouette Score:** Measures how well-separated the clusters are.  \n"
      ],
      "metadata": {
        "id": "OSJuBhia5Rz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<a name='2'></a> Hierarchical Clustering"
      ],
      "metadata": {
        "id": "P9tAFOBZ8YJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Hierarchical clustering** is an unsupervised machine learning method that builds a hierarchy of clusters without requiring a predefined number of clusters. It is commonly used for data grouping and visualization.  \n",
        "\n",
        "##### Types of Hierarchical Clustering:  \n",
        "1. **Agglomerative (Bottom-Up)** – Each data point starts as its own cluster, and clusters are merged iteratively based on similarity.  \n",
        "2. **Divisive (Top-Down)** – The entire dataset starts as one cluster, which is then recursively split into smaller clusters.  \n",
        "\n",
        "##### Steps in Agglomerative Clustering:  \n",
        "1. **Compute Distance Matrix** – Calculate distances between all data points.  \n",
        "2. **Merge Closest Clusters** – Use a linkage criterion to combine the two closest clusters.  \n",
        "3. **Update Distance Matrix** – Recalculate distances between new and existing clusters.  \n",
        "4. **Repeat Steps 2-3** until only one cluster remains.  \n",
        "\n",
        "##### Linkage Methods:  \n",
        "- **Single Linkage:** Distance between the closest points of two clusters.  \n",
        "- **Complete Linkage:** Distance between the farthest points of two clusters.  \n",
        "- **Average Linkage:** Average distance between all points in two clusters.  \n",
        "- **Centroid Linkage:** Distance between cluster centroids.  \n",
        "\n",
        "##### Dendrogram:  \n",
        "A **dendrogram** is a tree-like diagram that visualizes the merging process and helps determine the optimal number of clusters by cutting the tree at an appropriate height.  \n",
        "\n"
      ],
      "metadata": {
        "id": "1r9EpHn28UHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P03RxFa9z-2X"
      },
      "outputs": [],
      "source": []
    }
  ]
}