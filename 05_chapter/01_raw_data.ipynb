{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKPCrwO8XPxA8UM+DkZJ96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidofitaly/notes_02_50_key_stats_ds/blob/main/05_chapter/01_raw_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Naïve Bayes Classifier"
      ],
      "metadata": {
        "id": "71B5NNZjvHMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Naïve Bayes is a probabilistic classification model based on **Bayes' theorem** with the assumption of feature independence.\n",
        "\n",
        "##### **Bayes' Theorem**  \n",
        "\n",
        "Bayes' theorem describes the relationship between conditional probabilities:  \n",
        "$$ P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)} $$  \n",
        "Where:  \n",
        "- $ P(A \\mid B) $ – **posterior probability** (after observing $ B $),  \n",
        "- $ P(B \\mid A) $ – **conditional probability** (likelihood of $ B $ given $ A $),  \n",
        "- $ P(A) $ – **prior probability** (before observing $ B $),  \n",
        "- $ P(B) $ – total probability of $ B $.\n",
        "\n",
        "##### **Independence Assumption**  \n",
        "\n",
        "For a feature set $ x_1, x_2, \\dots, x_n $, the model assumes conditional independence:  \n",
        "$$ P(x_1, x_2, \\dots, x_n \\mid C) = P(x_1 \\mid C) P(x_2 \\mid C) \\dots P(x_n \\mid C) $$  \n",
        "\n",
        "##### **Classification Rule**  \n",
        "\n",
        "For a given class $ C_k $, the posterior probability is computed as:  \n",
        "$$ P(C_k \\mid x_1, x_2, \\dots, x_n) = \\frac{P(x_1 \\mid C_k) P(x_2 \\mid C_k) \\dots P(x_n \\mid C_k) P(C_k)}{P(x_1, x_2, \\dots, x_n)} $$  \n",
        "The predicted class is the one with the highest posterior probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "peWj-vabuBQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminant Analysis"
      ],
      "metadata": {
        "id": "NSh0wYGHzOya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Discriminant Analysis is a technique used to classify observations into predefined classes based on predictor variables. It focuses on finding boundaries between classes by analyzing differences in means and variances.\n",
        "\n",
        "##### 1. Covariance\n",
        "\n",
        "#####In discriminant analysis, covariance represents the relationship between variables within each class. For **Linear Discriminant Analysis (LDA)**, we assume that all classes share the same covariance matrix, denoted $ \\Sigma $.\n",
        "\n",
        "#####Covariance matrix for class $ k $:\n",
        "\n",
        "$$\n",
        "\\Sigma_k = \\frac{1}{n_k - 1} \\sum_{i=1}^{n_k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ n_k $ is the number of observations in class $ k $,\n",
        "- $ x_i $ is the $ i $-th observation,\n",
        "- $ \\mu_k $ is the mean vector for class $ k $.\n",
        "\n",
        "##### 2. Discriminant Function\n",
        "\n",
        "#####The **discriminant function** is a linear function used to classify new observations. It is defined as:\n",
        "\n",
        "$$\n",
        "g_k(x) = \\mathbf{w_k}^T x + b_k\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ g_k(x) $ is the discriminant function for class $ k $,\n",
        "- $ \\mathbf{w_k} $ is the weight vector,\n",
        "- $ x $ is the predictor vector,\n",
        "- $ b_k $ is the bias term.\n",
        "\n",
        "##### 3. Weights of the Discriminant Function\n",
        "\n",
        "#####The **weights** reflect the importance of each predictor in classifying data. The weight vector $ \\mathbf{w_k} $ is calculated as:\n",
        "\n",
        "$$\n",
        "\\mathbf{w_k} = \\Sigma^{-1} (\\mu_k - \\mu)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\Sigma^{-1} $ is the inverse of the pooled covariance matrix,\n",
        "- $ \\mu_k $ is the mean vector for class $ k $,\n",
        "- $ \\mu $ is the overall mean vector of all classes.\n",
        "\n",
        "#####The bias term is calculated as:\n",
        "\n",
        "$$\n",
        "b_k = -\\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\ln(\\pi_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\pi_k $ is the prior probability of class $ k $.\n"
      ],
      "metadata": {
        "id": "ETEKXtrAzHIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "5QCyK593SNRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#####**Logistic regression** is a statistical method used for modeling the relationship between a dependent binary variable and one or more independent variables. It is used when the outcome variable is categorical, often with two classes (e.g., success/failure, yes/no).\n",
        "\n",
        "#####The model predicts the probability that a given observation belongs to a particular class (usually coded as 0 or 1).\n",
        "\n",
        "##### Key Concepts:\n",
        "\n",
        "1. **Logistic Function** (Sigmoid Function):\n",
        "   The logistic regression model uses the logistic function to predict probabilities. The logistic function is an S-shaped curve that outputs values between 0 and 1. The formula is:\n",
        "   $$\n",
        "   P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n",
        "   $$  \n",
        "   Where:\n",
        "   - $P(y=1|X)$ is the probability of the class being 1 (success).\n",
        "   - $\\beta_0$ is the intercept.\n",
        "   - $\\beta_1, \\dots, \\beta_n$ are the coefficients for the independent variables $X_1, X_2, \\dots, X_n$.\n",
        "   - $e$ is the base of the natural logarithm.\n",
        "\n",
        "2. **Log-Odds**:\n",
        "   The output of the linear combination $\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_n X_n$ is called the log-odds. The logistic regression model estimates the log-odds of the dependent variable being 1.\n",
        "\n",
        "3. **Odds**:\n",
        "   The odds represent the ratio of the probability of an event happening to the probability of it not happening. The odds of event $y=1$ can be expressed as:\n",
        "   $$\n",
        "   \\text{Odds} = \\frac{P(y=1|X)}{1 - P(y=1|X)}\n",
        "   $$\n",
        "\n",
        "4. **Maximum Likelihood Estimation (MLE)**:\n",
        "   The parameters $\\beta_0, \\beta_1, \\dots, \\beta_n$ are estimated using **Maximum Likelihood Estimation (MLE)**, which finds the values of the parameters that maximize the likelihood of the observed data.\n",
        "\n",
        "5. **Interpretation of Coefficients**:\n",
        "   - The coefficients $\\beta_1, \\dots, \\beta_n$ represent the change in the log-odds of the dependent variable per unit change in the respective independent variable.\n",
        "   - Exponentiating the coefficients gives the **odds ratio** (OR), which represents how the odds of the outcome change with a one-unit increase in the predictor variable.\n",
        "\n",
        "   The odds ratio for a predictor $X_i$ is given by:\n",
        "   $$\n",
        "   OR_i = e^{\\beta_i}\n",
        "   $$\n",
        "\n"
      ],
      "metadata": {
        "id": "wdxzobjs5OA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Classification Models"
      ],
      "metadata": {
        "id": "myfWLSmnIVV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Evaluating a classification model is essential to understand its performance. Various metrics help assess how well a model distinguishes between different classes.\n",
        "\n",
        "##### 1. Accuracy\n",
        "Accuracy measures the proportion of correctly classified instances in the dataset:\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "- $TP$ (True Positives): Correctly predicted positive cases.\n",
        "- $TN$ (True Negatives): Correctly predicted negative cases.\n",
        "- $FP$ (False Positives): Incorrectly predicted positive cases.\n",
        "- $FN$ (False Negatives): Incorrectly predicted negative cases.\n",
        "\n",
        "**Limitation:** Accuracy may be misleading when classes are imbalanced.\n",
        "\n",
        "##### 2. Confusion Matrix\n",
        "The confusion matrix summarizes model performance by comparing predicted vs. actual values.\n",
        "\n",
        "| Actual / Predicted | Positive (1) | Negative (0) |\n",
        "|--------------------|-------------|-------------|\n",
        "| **Positive (1)**   | TP          | FN          |\n",
        "| **Negative (0)**   | FP          | TN          |\n",
        "\n",
        "It helps calculate other classification metrics.\n",
        "\n",
        "##### 3. Sensitivity (Recall)\n",
        "Sensitivity (also called recall or **true positive rate**) measures how well the model identifies actual positive cases:\n",
        "$$\n",
        "\\text{Sensitivity} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "- High sensitivity means fewer false negatives.\n",
        "- Important for medical diagnosis and fraud detection.\n",
        "\n",
        "##### 4. Specificity\n",
        "Specificity (**true negative rate**) measures how well the model identifies actual negative cases:\n",
        "$$\n",
        "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
        "$$\n",
        "- High specificity means fewer false positives.\n",
        "- Useful when false positives are costly (e.g., legal cases).\n",
        "\n",
        "##### 5. Precision\n",
        "Precision (**positive predictive value**) measures the proportion of correctly predicted positive cases:\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "- High precision means fewer false positives.\n",
        "- Important when false positives have high costs (e.g., spam filtering).\n",
        "\n",
        "##### 6. ROC Curve\n",
        "The **Receiver Operating Characteristic (ROC) curve** plots the **true positive rate** (sensitivity) against the **false positive rate** (1 - specificity) for different classification thresholds.  \n",
        "- A **perfect classifier** reaches the top-left corner (100% sensitivity, 100% specificity).\n",
        "- The **diagonal line** represents a random classifier.\n",
        "\n",
        "##### 7. AUC (Area Under Curve)\n",
        "The **AUC** measures the area under the ROC curve. It indicates how well a model distinguishes between classes:\n",
        "- **AUC = 1** → Perfect classification.\n",
        "- **AUC = 0.5** → Random guessing.\n",
        "- **Higher AUC** means a better model.\n",
        "\n",
        "##### 8. Lift\n",
        "**Lift** measures how much better a model performs compared to random selection. It is defined as:\n",
        "$$\n",
        "\\text{Lift} = \\frac{\\text{Precision of the model}}{\\text{Baseline Precision}}\n",
        "$$\n",
        "- A **lift of 2** means the model is twice as effective as random guessing.\n",
        "- Used in marketing and fraud detection.\n"
      ],
      "metadata": {
        "id": "r5LoWrsdIH0G"
      }
    }
  ]
}